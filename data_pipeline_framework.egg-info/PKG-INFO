Metadata-Version: 2.4
Name: data-pipeline-framework
Version: 0.1.0
Summary: A flexible, extensible Python framework for building data processing pipelines with intelligent object conversion
Author-email: Anton Nils Persson <antonnilspersson@gmail.com>
License: CC-BY-NC-4.0
Project-URL: Homepage, https://github.com/antonnpersson/data-pipeline
Project-URL: Bug Reports, https://github.com/antonnpersson/data-pipeline/issues
Project-URL: Source, https://github.com/antonnpersson/data-pipeline
Keywords: data,pipeline,etl,csv,json,conversion,transformation
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Data Processing
Classifier: Topic :: Text Processing
Classifier: Topic :: Utilities
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE.md
Requires-Dist: typing-extensions>=4.0.0
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: isort>=5.0.0; extra == "dev"
Requires-Dist: flake8>=5.0.0; extra == "dev"
Requires-Dist: mypy>=1.0.0; extra == "dev"
Provides-Extra: test
Requires-Dist: pytest>=7.0.0; extra == "test"
Requires-Dist: pytest-cov>=4.0.0; extra == "test"
Requires-Dist: pytest-mock>=3.0.0; extra == "test"
Provides-Extra: full
Requires-Dist: pandas>=1.3.0; extra == "full"
Requires-Dist: numpy>=1.21.0; extra == "full"
Requires-Dist: requests>=2.25.0; extra == "full"
Requires-Dist: openpyxl>=3.0.0; extra == "full"
Requires-Dist: lxml>=4.6.0; extra == "full"
Dynamic: license-file

# Data Pipeline Framework

A flexible, extensible Python framework for building data processing pipelines. Load data from any source, transform it however you want, and intelligently convert it to custom objects with zero configuration.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: CC BY-NC 4.0](https://img.shields.io/badge/License-CC%20BY--NC%204.0-lightgrey.svg)](https://creativecommons.org/licenses/by-nc/4.0/)

## ğŸš€ Quick Start

```bash
pip install data-pipeline-framework
```

```python
from dataclasses import dataclass
from data_pipeline import DataPipeline, SmartConverter
from data_pipeline.pipeline.loaders import CSVLoader
from data_pipeline.pipeline.parsers import CSVParser
from data_pipeline.pipeline.transformers import AutoCategorizerTransformer, DataCleanerTransformer

@dataclass
class Question:
    text: str
    category: str
    difficulty: int
    answers: List[str]
    correct_answer: str

# Create your pipeline - works with ANY CSV structure!
pipeline = DataPipeline(
    loader=CSVLoader(),
    parser=CSVParser(),
    transformers=[
        DataCleanerTransformer(),
        AutoCategorizerTransformer()
    ],
    converter=SmartConverter(Question)  # Automatically maps columns!
)

# Process your data - no matter how columns are named
questions = pipeline.execute('my_data.csv')
print(f"Processed {len(questions)} items!")
```

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Installation](#installation)
- [Core Concepts](#core-concepts)
- [Built-in Components](#built-in-components)
- [Usage Examples](#usage-examples)
- [Creating Custom Components](#creating-custom-components)
- [Advanced Features](#advanced-features)
- [API Reference](#api-reference)
- [Contributing](#contributing)
- [License](#license)

## ğŸ¯ Overview

This framework follows a simple but powerful pipeline pattern:

```
Source â†’ Loader â†’ Parser â†’ Transformers â†’ Smart Converter â†’ Custom Objects
```

**What makes this special?**
- ğŸ§  **Smart Conversion**: Automatically maps any column names to your object fields
- âœ… **Zero Configuration**: Works with any reasonable CSV/data structure
- âœ… **Type Intelligence**: Converts values to correct types automatically
- âœ… **Modular**: Mix and match components
- âœ… **Extensible**: Easy to add custom logic
- âœ… **Type-safe**: Full type hints and generics
- âœ… **Chainable**: Multiple transformers in sequence
- âœ… **Universal**: Same framework works for any custom object

## ğŸ“¦ Installation

### From PyPI (Recommended)
```bash
pip install data-pipeline-framework
```

### From Source
```bash
git clone https://github.com/yourusername/data-pipeline-framework.git
cd data-pipeline-framework
pip install -e .
```

### Development Installation
```bash
git clone https://github.com/yourusername/data-pipeline-framework.git
cd data-pipeline-framework
pip install -e ".[dev]"
```

## ğŸ§  Core Concepts

### Pipeline Components

1. **Loader**: Loads raw data from any source (files, APIs, databases)
2. **Parser**: Converts raw data into tabular format (list of dictionaries)
3. **Transformers**: Manipulate, enrich, or clean the data (chainable!)
4. **Smart Converter**: Intelligently maps tabular data to your custom objects

### Key Classes

```python
from data_pipeline import BaseLoader, BaseParser, BaseTransformer, BaseConverter, SmartConverter

class MyLoader(BaseLoader):
    def load(self, source: str, **kwargs) -> Any: ...
    def validate_source(self, source: str) -> bool: ...

class MyTransformer(BaseTransformer):
    def transform(self, data: List[Dict], **kwargs) -> List[Dict]: ...
    def get_description(self) -> str: ...

# The magic happens here - no manual mapping needed!
converter = SmartConverter(MyCustomClass)
```

### Smart Conversion Examples

The `SmartConverter` automatically maps columns to object fields:

```python
# Your CSV columns can be named ANYTHING:
"Question Text" â†’ text
"Topic" â†’ category
"Difficulty Level" â†’ difficulty
"Answer Options" â†’ answers
"Correct Answer" â†’ correct_answer

# All of these work automatically:
"Q", "question", "prompt" â†’ text
"cat", "subject", "type" â†’ category
"level", "hard", "complexity" â†’ difficulty
```

## ğŸ›  Built-in Components

### Loaders
- `CSVLoader` - Load CSV files
- `JSONLoader` - Load JSON files
- `APILoader` - Load data from REST APIs
- `DatabaseLoader` - Load from SQL databases

### Parsers
- `CSVParser` - Parse CSV data
- `JSONParser` - Parse JSON data
- `XMLParser` - Parse XML data

### Transformers
- `DataCleanerTransformer` - Clean and normalize data
- `AutoCategorizerTransformer` - Auto-categorize based on keywords
- `DifficultyAnalyzerTransformer` - Analyze text complexity
- `DeduplicatorTransformer` - Remove duplicate entries
- `TranslatorTransformer` - Translate text fields
- `SentimentAnalyzerTransformer` - Add sentiment analysis

### Mappers
- `QuestionMapper` - Map to Question objects
- `GenericMapper` - Map to any dataclass

## ğŸ“š Usage Examples

### Universal Smart Conversion

```python
from dataclasses import dataclass
from data_pipeline import DataPipeline, SmartConverter
from data_pipeline.pipeline.loaders import CSVLoader
from data_pipeline.pipeline.parsers import CSVParser

@dataclass
class Product:
    name: str
    price: float
    category: str
    in_stock: bool

# No manual mapping needed! SmartConverter figures it out
pipeline = DataPipeline(
    loader=CSVLoader(),
    parser=CSVParser(),
    transformers=[],
    converter=SmartConverter(Product)  # Works with ANY object!
)

# This CSV with ANY column names works:
# "Product Name", "Cost", "Type", "Available" 
# "item", "price", "cat", "stock"
# "name", "amount", "category", "in_stock"
products = pipeline.execute('products.csv')
```

### Working with Any Data Structure

```python
# Define your domain object
@dataclass
class Question:
    text: str
    category: str
    difficulty: int
    answers: List[str]
    correct_answer: str

# The converter handles ALL of these CSV variations automatically:

# Variation 1:
# "Question Text", "Topic", "Level", "Options", "Answer"

# Variation 2: 
# "Q", "Subject", "Difficulty", "Choices", "Correct"

# Variation 3:
# "question", "cat", "hard", "answers", "solution"

converter = SmartConverter(Question)
# All variations work with zero configuration!
```

### Advanced Example with Multiple Transformers

```python
from data_pipeline import DataPipeline, PipelineConfig, SmartConverter
from data_pipeline.pipeline.transformers import (
    DataCleanerTransformer,
    AutoCategorizerTransformer,
    DifficultyAnalyzerTransformer
)

@dataclass
class Question:
    text: str
    category: str
    difficulty: int
    answers: List[str]
    correct_answer: str

# Configure custom categorization
categorizer = AutoCategorizerTransformer(
    category_keywords={
        'science': ['physics', 'chemistry', 'biology'],
        'history': ['ancient', 'war', 'civilization'],
        'sports': ['football', 'tennis', 'basketball']
    }
)

# Create pipeline with transformation chain
pipeline = DataPipeline(
    loader=CSVLoader(),
    parser=CSVParser(),
    transformers=[
        DataCleanerTransformer(),      # Clean first
        categorizer,                   # Then categorize
        DifficultyAnalyzerTransformer() # Finally analyze difficulty
    ],
    converter=SmartConverter(Question)  # Smart conversion!
)

# Configure with custom parameters
config = PipelineConfig(
    parser_kwargs={'delimiter': ';'},
    transformer_kwargs={'strict_validation': True},
    converter_kwargs={'confidence_threshold': 0.8}
)

questions = pipeline.execute('quiz_data.csv', config)
```

### Type-Aware Conversion

```python
@dataclass
class SmartObject:
    name: str
    count: int           # "5" â†’ 5
    price: float         # "19.99" â†’ 19.99
    active: bool         # "yes" â†’ True, "1" â†’ True
    tags: List[str]      # "red|blue|green" â†’ ["red", "blue", "green"]
    description: Optional[str] = None

# SmartConverter handles all type conversions automatically
converter = SmartConverter(SmartObject)
# Works with messy data, missing fields, different formats
```

### Working with APIs

```python
from data_pipeline.pipeline.loaders import APILoader
from data_pipeline.pipeline.parsers import JSONParser

# Load data from REST API
pipeline = DataPipeline(
    loader=APILoader(),
    parser=JSONParser(),
    transformers=[DataCleanerTransformer()],
    converter=SmartConverter(Product)  # Works with any data source!
)

# Execute with API endpoint
products = pipeline.execute('https://api.example.com/products')
```

### Multiple Object Types

```python
# Same framework, different objects
@dataclass
class Person:
    name: str
    email: str
    age: int
    
@dataclass  
class Order:
    id: str
    customer: str
    total: float
    items: List[str]

# Create pipelines for different data types
people_pipeline = DataPipeline(..., converter=SmartConverter(Person))
orders_pipeline = DataPipeline(..., converter=SmartConverter(Order))

# Both work automatically with appropriate CSV files
people = people_pipeline.execute('contacts.csv')
orders = orders_pipeline.execute('sales.csv')
```

## ğŸ”§ Creating Custom Components

### Custom Loader

```python
from data_pipeline import BaseLoader
import requests

class APILoader(BaseLoader):
    def load(self, source: str, **kwargs) -> dict:
        headers = kwargs.get('headers', {})
        response = requests.get(source, headers=headers)
        response.raise_for_status()
        return response.json()
    
    def validate_source(self, source: str) -> bool:
        return source.startswith(('http://', 'https://'))
```

### Custom Transformer

```python
from data_pipeline import BaseTransformer
import re

class EmailValidatorTransformer(BaseTransformer):
    def transform(self, data, **kwargs):
        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$'
        
        for item in data:
            email = item.get('email', '')
            item['email_valid'] = bool(re.match(email_pattern, email))
        
        return data
    
    def get_description(self):
        return "Validates email addresses and adds email_valid field"
```

### Custom Converter

```python
from data_pipeline import BaseConverter
from typing import Type, List, Dict, Any

class CustomConverter(BaseConverter[MyObject]):
    def convert(self, data: List[Dict[str, Any]], **kwargs) -> List[MyObject]:
        # Custom conversion logic
        results = []
        for item in data:
            obj = MyObject(
                # Your custom mapping logic here
                special_field=self.custom_transform(item)
            )
            results.append(obj)
        return results
    
    def get_target_type(self) -> Type[MyObject]:
        return MyObject
    
    def suggest_field_mapping(self, columns: List[str]) -> Dict[str, str]:
        # Your custom field mapping logic
        return {'target_field': 'source_column'}
```

### Advanced Smart Converter

```python
from data_pipeline import SmartConverter

class EnhancedConverter(SmartConverter[Question]):
    def __init__(self):
        super().__init__(Question)
        # Add custom field aliases
        self.field_aliases.update({
            'text': ['question', 'q', 'prompt', 'query', 'problem'],
            'difficulty': ['level', 'hard', 'complexity', 'diff', 'grade']
        })
    
    def _convert_value(self, value: Any, target_type: type) -> Any:
        # Override conversion for special cases
        if target_type == int and isinstance(value, str):
            difficulty_map = {'easy': 1, 'medium': 2, 'hard': 3}
            return difficulty_map.get(value.lower(), super()._convert_value(value, target_type))
        
        return super()._convert_value(value, target_type)
```

### Register Your Components

```python
from data_pipeline import registry

# Register your custom components
registry.register_loader('api', APILoader)
registry.register_transformer('email_validator', EmailValidatorTransformer)
registry.register_converter('enhanced_question', EnhancedConverter)

# Create pipeline from registry
pipeline = registry.create_pipeline(
    loader_name='api',
    parser_name='json',
    transformer_names=['data_cleaner', 'email_validator'],
    converter_name='enhanced_question'
)
```

## ğŸš€ Advanced Features

### Error Handling

```python
import logging

# Configure logging
logging.basicConfig(level=logging.INFO)

try:
    results = pipeline.execute('data.csv')
except ValueError as e:
    print(f"Invalid source: {e}")
except Exception as e:
    print(f"Pipeline failed: {e}")
```

### Custom Configuration

```python
from data_pipeline import PipelineConfig

config = PipelineConfig(
    loader_kwargs={
        'encoding': 'utf-8',
        'timeout': 30
    },
    parser_kwargs={
        'delimiter': ',',
        'quotechar': '"'
    },
    transformer_kwargs={
        'strict_mode': True,
        'skip_errors': False
    },
    converter_kwargs={
        'confidence_threshold': 0.8,  # Minimum field mapping confidence
        'strict_types': True,          # Enforce type conversion
        'skip_invalid': False          # Skip rows that can't be converted
    }
)

results = pipeline.execute('data.csv', config)
```

### Field Mapping Insights

```python
# See what mappings the SmartConverter suggests
converter = SmartConverter(Question)
columns = ["Question Text", "Topic", "Level", "Answer Options", "Correct Answer"]
mapping = converter.suggest_field_mapping(columns)
print(f"Suggested mapping: {mapping}")
# Output: {'text': 'Question Text', 'category': 'Topic', 'difficulty': 'Level', ...}

# Override mapping if needed
custom_mapping = {'text': 'Question Text', 'category': 'Subject'}
converter.field_mapping = custom_mapping
```

### Conditional Transformers

```python
class ConditionalTransformer(BaseTransformer):
    def __init__(self, condition_field: str, condition_value: str):
        self.condition_field = condition_field
        self.condition_value = condition_value
    
    def transform(self, data, **kwargs):
        for item in data:
            if item.get(self.condition_field) == self.condition_value:
                # Apply transformation only if condition is met
                item['special_flag'] = True
        return data
```

## ğŸ“– API Reference

### DataPipeline

```python
class DataPipeline(Generic[T]):
    def __init__(self, loader, parser, transformers, converter): ...
    def execute(self, source: str, config: PipelineConfig = None) -> List[T]: ...
```

### PipelineConfig

```python
@dataclass
class PipelineConfig:
    loader_kwargs: Dict[str, Any] = None
    parser_kwargs: Dict[str, Any] = None
    transformer_kwargs: Dict[str, Any] = None
    converter_kwargs: Dict[str, Any] = None  # New!
```

### SmartConverter

```python
class SmartConverter(BaseConverter[T]):
    def __init__(self, target_class: Type[T]): ...
    def convert(self, data: List[Dict], **kwargs) -> List[T]: ...
    def suggest_field_mapping(self, columns: List[str]) -> Dict[str, str]: ...
    def get_target_type(self) -> Type[T]: ...
```

### Registry

```python
class PipelineRegistry:
    def register_loader(self, name: str, loader_class: type): ...
    def register_parser(self, name: str, parser_class: type): ...
    def register_transformer(self, name: str, transformer_class: type): ...
    def register_converter(self, name: str, converter_class: type): ...  # New!
    def create_pipeline(self, loader_name, parser_name, transformer_names, converter_name): ...
```

## ğŸ§ª Testing

```bash
# Run tests
pytest

# Run with coverage
pytest --cov=data_pipeline

# Run specific test
pytest tests/test_transformers.py::test_auto_categorizer
```

### Example Test

```python
def test_smart_converter():
    from data_pipeline import SmartConverter
    
    @dataclass
    class TestObject:
        name: str
        count: int
        active: bool
    
    converter = SmartConverter(TestObject)
    
    # Test field mapping
    columns = ["Product Name", "Quantity", "Is Active"]
    mapping = converter.suggest_field_mapping(columns)
    assert mapping['name'] == 'Product Name'
    assert mapping['count'] == 'Quantity'
    assert mapping['active'] == 'Is Active'
    
    # Test conversion
    data = [
        {"Product Name": "Widget", "Quantity": "5", "Is Active": "yes"}
    ]
    result = converter.convert(data)
    
    assert len(result) == 1
    assert result[0].name == "Widget"
    assert result[0].count == 5
    assert result[0].active == True
```

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Setup

```bash
git clone https://github.com/yourusername/data-pipeline-framework.git
cd data-pipeline-framework
pip install -e ".[dev]"
pre-commit install
```

### Code Style

We use `black` for code formatting and `mypy` for type checking:

```bash
black src/
mypy src/
```

## ğŸ“ License

This project is licensed under the CC BY-NC 4.0 License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Inspired by modern data engineering pipelines
- Built with type safety and extensibility in mind
- Special thanks to all contributors

## ğŸ“ Support

- ğŸ“š [Documentation](https://data-pipeline-framework.readthedocs.io/)
- ğŸ› [Issue Tracker](https://github.com/yourusername/data-pipeline-framework/issues)
- ğŸ’¬ [Discussions](https://github.com/yourusername/data-pipeline-framework/discussions)
- ğŸ“§ Email: support@data-pipeline-framework.com

---

**Happy pipelining! ğŸš€**
